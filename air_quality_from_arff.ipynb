{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# City Day Data Analysis from Zip File",
"",
"This notebook is designed to automatically load and process a .zip file containing air quality data. It includes steps for:",
"",
"1.  Data Loading: Extracts a CSV file from a zip archive and loads it into a Pandas DataFrame.",
"2.  Exploratory Data Analysis (EDA): Provides a quick overview of the data, including a description, and checks for missing values.",
"3.  Preprocessing & Model Training: Sets up a machine learning pipeline to handle both numeric and categorical features and trains a baseline RandomForest model.",
"4.  Evaluation: Reports key metrics to assess the model's performance."
]
},
{
"cell_type": "code",
"metadata": {},
"outputs": [],
"source": [
"# --- 1. Imports & Setup ---",
"# Required libraries for data manipulation, machine learning, and file handling.",
"import os, re, io, json, zipfile",
"import numpy as np",
"import pandas as pd",
"",
"from sklearn.model_selection import train_test_split",
"from sklearn.compose import ColumnTransformer",
"from sklearn.preprocessing import OneHotEncoder, StandardScaler",
"from sklearn.pipeline import Pipeline",
"from sklearn.metrics import classification_report, accuracy_score, f1_score, mean_squared_error, r2_score",
"from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor",
"",
"# Define the path to the ZIP file. This is the correct path for the file in this environment.",
"ZIP_PATH = r"/mnt/data/archive (1).zip"",
"CSV_FILE_NAME = "city_day.csv"",
"TARGET_FALLBACK = "AQI_Bucket" # We will set the target to AQI_Bucket for classification.",
"",
"print("Zip file path:", ZIP_PATH)"
]
},
{
"cell_type": "code",
"metadata": {},
"outputs": [],
"source": [
"# --- 2. Data Loading and Extraction ---",
"# This function extracts the CSV from the zip file and loads it into a DataFrame.",
"def load_csv_from_zip(zip_path, file_name):",
"    try:",
"        with zipfile.ZipFile(zip_path, 'r') as zip_ref:",
"            # Extract the specified CSV file to a temporary location.",
"            zip_ref.extract(file_name, ".")",
"        # Read the extracted CSV file into a pandas DataFrame.",
"        df = pd.read_csv(file_name)",
"        return df",
"    except FileNotFoundError:",
"        print(f"Error: The file '{zip_path}' or '{file_name}' was not found.")",
"        return None",
"    except Exception as e:",
"        print(f"An error occurred while loading the data: {e}")",
"        return None",
"",
"df = load_csv_from_zip(ZIP_PATH, CSV_FILE_NAME)",
"if df is not None:",
"    print("Shape:", df.shape)",
"    print("Columns (first 10):", list(df.columns)[:10], "..." if df.shape[1] > 10 else "")",
"    display(df.head())",
"else:",
"    print("Could not load data. Please check the file path and name.")"
]
},
{
"cell_type": "code",
"metadata": {},
"outputs": [],
"source": [
"# --- 3. Exploratory Data Analysis (EDA) ---",
"# Automatically detect the target column, and display data summaries.",
"",
"if df is None:",
"    raise ValueError("DataFrame is empty, cannot proceed with analysis.")",
"",
"# Use the TARGET_FALLBACK if provided, otherwise default to a column named 'class' or the last column.",
"cols_lower = {c.lower(): c for c in df.columns}",
"if TARGET_FALLBACK and TARGET_FALLBACK in df.columns:",
"    target_col = TARGET_FALLBACK",
"elif "class" in cols_lower:",
"    target_col = cols_lower["class"]",
"else:",
"    target_col = df.columns[-1] if len(df.columns) else None",
"",
"print("Target column selected:", target_col)",
"",
"display(df.describe(include="all"))",
"print("Missing values (top 20):")",
"print(df.isna().sum().sort_values(ascending=False).head(20))"
]
},
{
"cell_type": "code",
"metadata": {},
"outputs": [],
"source": [
"# --- 4. Data Splitting and Model Training ---",
"# Prepare the data for a machine learning model. It will automatically detect if it's a classification or regression task.",
"",
"if target_col is None:",
"    raise ValueError("Could not determine a target column. Please set target_col manually.")",
"",
"X = df.drop(columns=[target_col]).copy()",
"y = df[target_col].copy()",
"",
"# Determine if the task is classification or regression based on the target column.",
"is_numeric_target = pd.api.types.is_numeric_dtype(y)",
"n_unique = y.nunique(dropna=True)",
"task = "classification"",
"if is_numeric_target and n_unique > 15:",
"    task = "regression"",
"print(f"Detected task: {task} (unique target values: {n_unique})")",
"",
"# Drop rows with missing values in the target column to prevent errors.",
"valid_indices = y.dropna().index",
"X = X.loc[valid_indices]",
"y = y.loc[valid_indices]",
"X_train, X_test, y_train, y_test = train_test_split(",
"    X, y, test_size=0.2, random_state=42,",
"    stratify=y if task=="classification" and n_unique>1 else None",
")",
"",
"# Identify numeric and categorical columns for preprocessing.",
"num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]",
"cat_cols = [c for c in X.columns if c not in num_cols]",
"",
"# Create a preprocessing pipeline with StandardScaler for numeric data and OneHotEncoder for categorical data.",
"preprocess = ColumnTransformer(",
"    transformers=[",
"        ("num", StandardScaler(with_mean=False), num_cols),",
"        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),",
"    ]",
")",
"",
"# Choose the appropriate model based on the task.",
"if task == "classification":",
"    model = RandomForestClassifier(n_estimators=200, random_state=42)",
"else:",
"    model = RandomForestRegressor(n_estimators=200, random_state=42)",
"",
"# Create the full pipeline and fit the model.",
"pipe = Pipeline(steps=[("prep", preprocess), ("model", model)])",
"pipe.fit(X_train, y_train)"
]
},
{
"cell_type": "code",
"metadata": {},
"outputs": [],
"source": [
"# --- 5. Model Evaluation ---",
"# Evaluate the model's performance on the test set.",
"",
"if task == "classification":",
"    pred = pipe.predict(X_test)",
"    print("Accuracy:", accuracy_score(y_test, pred))",
"    print("F1 (macro):", f1_score(y_test, pred, average="macro"))",
"    print("\nClassification report:\n", classification_report(y_test, pred))",
"else:",
"    pred = pipe.predict(X_test)",
"    rmse = mean_squared_error(y_test, pred, squared=False)",
"    r2 = r2_score(y_test, pred)",
"    print("RMSE:", rmse)",
"    print("R^2:", r2)"
]
},
{
"cell_type": "code",
"metadata": {},
"outputs": [],
"source": [
"# --- 6. (Optional) Permutation Importance ---",
"# Calculate and display feature importance to understand which features contribute most to the model's predictions.",
"try:",
"    from sklearn.inspection import permutation_importance",
"    r = permutation_importance(pipe, X_test, y_test, n_repeats=5, random_state=42, scoring=None)",
"    cat_encoder = pipe.named_steps["prep"].transformers_[1][1]",
"    cat_feature_names = list(cat_encoder.get_feature_names_out(cat_cols)) if len(cat_cols) else []",
"    feature_names = num_cols + cat_feature_names",
"    import pandas as pd",
"    importances = pd.Series(r.importances_mean, index=feature_names)",
"    display(importances.sort_values(ascending=False).head(20))",
"except Exception as e:",
"    print("Permutation importance skipped. This may be due to missing libraries or a non-compatible model. Error:", e)"
]
},
{
"cell_type": "code",
"metadata": {},
"outputs": [],
"source": [
"# --- 7. (Optional) Save a Raw CSV Copy ---",
"# Save a cleaned version of the data to a CSV file for future use.",
"clean_csv_path = "/mnt/data/city_day_raw.csv"",
"if df is not None:",
"    df.to_csv(clean_csv_path, index=False)",
"    print("Saved raw CSV to:", clean_csv_path)"
]
}
],
"metadata": {
"language_info": {
"name": "python"
}
},
"nbformat": 4,
"nbformat_minor": 5
}
